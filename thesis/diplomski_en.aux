\relax 
\@nameuse{bbl@beforestart}
\babel@aux{english}{}
\citation{henisch1996photograph}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{7}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:introduction}{{1}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Hand-colored photograph by Stillfried \& Andersen (made the period between 1862 and 1885)\relax }}{7}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:hand_colored}{{1.1}{7}}
\newlabel{fig:stone_house:original}{{1.2a}{8}}
\newlabel{sub@fig:stone_house:original}{{a}{8}}
\newlabel{fig:stone_house:grayscale}{{1.2b}{8}}
\newlabel{sub@fig:stone_house:grayscale}{{b}{8}}
\newlabel{fig:stone_house:colorized}{{1.2c}{8}}
\newlabel{sub@fig:stone_house:colorized}{{c}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces Colorization example on an image of a stone house\relax }}{8}\protected@file@percent }
\newlabel{fig:stone_house}{{1.2}{8}}
\citation{reinhard2001colorization}
\citation{welsh2002colorization}
\citation{levin2004colorization}
\citation{luan2007colorization}
\citation{charpiat2008colorization}
\citation{cheng2015colorization}
\citation{deshpande2015colorization}
\citation{zhang2017ideep}
\citation{guadarrama2017colorization}
\citation{nazeri2018gan}
\citation{kumar2021colorization}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Related work}{9}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\citation{pang2021imagetoimage}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Base}{10}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:base}{{3}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces  Colored image of Alim Khan taken by Sergey Prokudin-Gorsky in 1911 using the three-image color photography method on the left, and the green, red, and blue (top to bottom) filter negatives (shown as positives) on the right.\relax }}{10}\protected@file@percent }
\newlabel{fig:alim_khan}{{3.1}{10}}
\newlabel{eq:colorization}{{3.1}{10}}
\citation{icc2004cielab}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Color Space}{11}\protected@file@percent }
\newlabel{sec:colorspace}{{3.1}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Slices of the CIELAB color space at different luminance levels, with only the RGB gamut rendered\relax }}{11}\protected@file@percent }
\newlabel{fig:rgb_in_lab}{{3.2}{11}}
\newlabel{eq:colorization_2}{{3.2}{11}}
\citation{levin2004colorization}
\citation{luan2007colorization}
\citation{charpiat2008colorization}
\citation{reinhard2001colorization}
\citation{cheng2015colorization}
\citation{iizuka2016colorization}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Image with separated channels of the CIELAB color space\relax }}{12}\protected@file@percent }
\newlabel{fig:lab_channels}{{3.3}{12}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Task Definition}{12}\protected@file@percent }
\newlabel{sec:task}{{3.2}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Multimodality}{12}\protected@file@percent }
\newlabel{sec:multimodality}{{3.2.1}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Image of three scooters, colorizer input, and possible colorization\relax }}{13}\protected@file@percent }
\newlabel{fig:multimodality_scooters}{{3.4}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Image of the sea taken during sunset, colorizer input, and possible colorization\relax }}{13}\protected@file@percent }
\newlabel{fig:multimodality_sunset}{{3.5}{13}}
\citation{fukushima1980neocognitron}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces Comparison of biological neuron and artificial neurons (stronger colors indicate higher weights)\relax }}{14}\protected@file@percent }
\newlabel{fig:neuron}{{3.6}{14}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Artificial Neural Networks}{14}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Convolutional Neural Networks}{14}\protected@file@percent }
\newlabel{sec:cnn}{{3.3.1}{14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}U-net}{15}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces Example of a U-Net with four downsampling and four upsampling blocks where the skip-connection is accomplished by concatenating outputs of the encoder to the upsampled features of the decoder\relax }}{15}\protected@file@percent }
\newlabel{fig:unet}{{3.7}{15}}
\citation{deng2009imagenet}
\citation{fei2004caltech}
\citation{zhou2017places}
\citation{krizhevskycifar}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Dataset}{16}\protected@file@percent }
\newlabel{sec:dataset}{{3.4}{16}}
\citation{zhang2016colorful}
\citation{simonyan2015vgg}
\citation{abien2018relu}
\citation{ioffe2015batchnorm}
\citation{yu2016atrous}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Models}{17}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Colorful Image Colorization}{17}\protected@file@percent }
\newlabel{sec:colorful}{{4.1}{17}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Architecture}{17}\protected@file@percent }
\newlabel{sec:colorful_architecture}{{4.1.1}{17}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Quantized \textbf  {a*b*} bins for different \textbf  {L*} values\relax }}{17}\protected@file@percent }
\newlabel{fig:lab_bins}{{4.1}{17}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces The architecture of the \textit  {Colorful Image Colorization} model. The eight convolutional blocks and the layer mapping the output to the probability distribution over the $Q$-bins are colored in yellow, and probability to \textbf  {a*b*} mapping in black\relax }}{18}\protected@file@percent }
\newlabel{fig:architecture_colorful}{{4.2}{18}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}Annealed mean}{18}\protected@file@percent }
\newlabel{eq:annealed_mean}{{4.1}{18}}
\newlabel{eq:weighed sum}{{4.2}{19}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Colorization of an image using different values of $T$\relax }}{19}\protected@file@percent }
\newlabel{fig:temperature}{{4.3}{19}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.3}Training}{19}\protected@file@percent }
\newlabel{sec:colorful_training}{{4.1.3}{19}}
\citation{diederik2015adam}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces Original image on the left, and the same image where the resolution of the \textbf  {a*} and \textbf  {b*} channels was reduced by a factor of four on the right\relax }}{20}\protected@file@percent }
\newlabel{fig:color4}{{4.4}{20}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces  The image is first resized to a size where the shorter of the two sides is $256$ pixels in length, afterwards a $256\times 256$ square is randomly cropped out of the resized image\relax }}{20}\protected@file@percent }
\newlabel{fig:crop}{{4.5}{20}}
\citation{zhang2016colorful}
\citation{zhang2016colorful}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Training step for the \textit  {Colorful Image Colorization} model\relax }}{21}\protected@file@percent }
\newlabel{alg:colorful}{{1}{21}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces Graph borrowed from the original paper \citep  {zhang2016colorful}, showing the overall distribution of color channel values in the ImageNet dataset, using a logarithmic scale (red indicates high, and blue low occurrence)\relax }}{21}\protected@file@percent }
\newlabel{fig:distribution}{{4.6}{21}}
\citation{tantithamthavorn2020rebalancing}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.4}Conclusion and gallery}{22}\protected@file@percent }
\citation{goodfellow2014generative}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Generative Adversarial Networks for Colorization}{24}\protected@file@percent }
\newlabel{sec:gan}{{4.2}{24}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Architecture}{24}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Generative adversarial networks}{24}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.13}{\ignorespaces Analogy for the way GANs work\relax }}{24}\protected@file@percent }
\newlabel{fig:museum}{{4.13}{24}}
\citation{mirza2014cgan}
\citation{lecun2010mnist}
\citation{goodfellow2014generative}
\@writefile{lof}{\contentsline {figure}{\numberline {4.14}{\ignorespaces A successful result of a GAN trained on the MNIST dataset on the left, and an unsuccessful attempt that ended up in mode collapse on the right\relax }}{25}\protected@file@percent }
\newlabel{fig:gan_mnist}{{4.14}{25}}
\@writefile{toc}{\contentsline {subsubsection}{Conditional GAN}{25}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.15}{\ignorespaces Generating fake hand-written digits based on the MNIST dataset using the cGAN framework. Digits in the same row are generated using the same latent vector (seed) and demonstrate similar characteristics\relax }}{26}\protected@file@percent }
\newlabel{fig:cgan_mnist}{{4.15}{26}}
\@writefile{toc}{\contentsline {subsubsection}{Generator}{26}\protected@file@percent }
\citation{isola2017pix2pix}
\@writefile{lof}{\contentsline {figure}{\numberline {4.16}{\ignorespaces The U-Net architecture of the model used for the generator, the short connection is done by concatenating the features from the encoder to the de-convoluted features of the previous layer in the decoder.\relax }}{27}\protected@file@percent }
\newlabel{fig:architecture_gan}{{4.16}{27}}
\@writefile{toc}{\contentsline {subsubsection}{Discriminator}{27}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.17}{\ignorespaces The patch-GAN discriminator architecture\relax }}{27}\protected@file@percent }
\newlabel{fig:architecture_gan_discriminator}{{4.17}{27}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.18}{\ignorespaces The patch-GAN discriminator architecture\relax }}{28}\protected@file@percent }
\newlabel{fig:architecture_gan_full}{{4.18}{28}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Training}{28}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Training step for the \textit  {Colorful Image Colorization} model\relax }}{28}\protected@file@percent }
\newlabel{alg:gan}{{2}{28}}
\citation{goodfellow2017nips}
\citation{salimans2016improved_gans}
\@writefile{lof}{\contentsline {figure}{\numberline {4.19}{\ignorespaces Healthy back and forth between the generator and the discriminator. Discriminator error on real samples in red, and fake samples in blue.\relax }}{29}\protected@file@percent }
\newlabel{fig:gan_plot}{{4.19}{29}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.3}Conclusion and gallery}{30}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Interactive Deep Colorization}{32}\protected@file@percent }
\newlabel{sec:ideep}{{4.3}{32}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Architecture}{32}\protected@file@percent }
\newlabel{sec:ideep_architecture}{{4.3.1}{32}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.26}{\ignorespaces The architecture of the \textit  {Interactive Deep Colorization} network. The base \textit  {Colorful Image Colorization} network is highlighted in red, the new layers are yellow, and the global hints network is colored purple.\relax }}{32}\protected@file@percent }
\newlabel{fig:architecture_ideep}{{4.26}{32}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}Global hints}{33}\protected@file@percent }
\newlabel{sec:global}{{4.3.2}{33}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.3}Training}{33}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.4}Conclusion and gallery}{34}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Conclusion}{36}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Colorization failure cases\relax }}{36}\protected@file@percent }
\newlabel{fig:fail}{{5.1}{36}}
\bibdata{literatura}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Result Comparison}{37}\protected@file@percent }
\bibcite{abien2018relu}{{1}{2018}{{Agarap}}{{}}}
\bibcite{charpiat2008colorization}{{2}{2008}{{Charpiat et~al.}}{{Charpiat, Hofmann, and Sch{\"o}lkopf}}}
\bibcite{cheng2015colorization}{{3}{2015}{{Cheng et~al.}}{{Cheng, Yang, and Sheng}}}
\bibcite{deng2009imagenet}{{4}{2009}{{Deng et~al.}}{{Deng, Dong, Socher, Li, Li, and Fei-Fei}}}
\bibcite{deshpande2015colorization}{{5}{2015}{{Deshpande et~al.}}{{Deshpande, Rock, and Forsyth}}}
\bibcite{fukushima1980neocognitron}{{6}{1980}{{Fukushima}}{{}}}
\bibcite{goodfellow2017nips}{{7}{2017}{{Goodfellow}}{{}}}
\bibcite{goodfellow2014generative}{{8}{2014}{{Goodfellow et~al.}}{{Goodfellow, Pouget-Abadie, Mirza, Xu, Warde-Farley, Ozair, Courville, and Bengio}}}
\bibcite{guadarrama2017colorization}{{9}{2017}{{Guadarrama et~al.}}{{Guadarrama, Dahl, Bieber, Norouzi, Shlens, and Murphy}}}
\bibcite{henisch1996photograph}{{10}{1996}{{Henisch and Henisch}}{{}}}
\bibcite{iizuka2016colorization}{{11}{2016}{{Iizuka et~al.}}{{Iizuka, Simo-Serra, and Ishikawa}}}
\@writefile{toc}{\contentsline {chapter}{Bibliography}{38}\protected@file@percent }
\bibcite{icc2004cielab}{{12}{2004}{{Int}}{{}}}
\bibcite{ioffe2015batchnorm}{{13}{2015}{{Ioffe and Szegedy}}{{}}}
\bibcite{isola2017pix2pix}{{14}{2017}{{Isola et~al.}}{{Isola, Zhu, Zhou, and Efros}}}
\bibcite{diederik2015adam}{{15}{2015}{{Kingma and Ba}}{{}}}
\bibcite{krizhevskycifar}{{16}{}{{Krizhevsky et~al.}}{{Krizhevsky, Nair, and Hinton}}}
\bibcite{kumar2021colorization}{{17}{2021}{{Kumar et~al.}}{{Kumar, Weissenborn, and Kalchbrenner}}}
\bibcite{fei2004caltech}{{18}{2004}{{L.~Fei-Fei and Perona}}{{}}}
\bibcite{lecun2010mnist}{{19}{}{{LeCun and Cortes}}{{}}}
\bibcite{levin2004colorization}{{20}{2004}{{Levin et~al.}}{{Levin, Lischinski, and Weiss}}}
\bibcite{luan2007colorization}{{21}{2007}{{Luan et~al.}}{{Luan, Wen, Cohen-Or, Liang, Xu, and Shum}}}
\bibcite{mirza2014cgan}{{22}{2014}{{Mirza and Osindero}}{{}}}
\bibcite{nazeri2018gan}{{23}{2018}{{Nazeri and Ng}}{{}}}
\bibcite{pang2021imagetoimage}{{24}{2021}{{Pang et~al.}}{{Pang, Lin, Qin, and Chen}}}
\bibcite{reinhard2001colorization}{{25}{2001}{{Reinhard et~al.}}{{Reinhard, Adhikhmin, Gooch, and Shirley}}}
\bibcite{salimans2016improved_gans}{{26}{2016}{{Salimans et~al.}}{{Salimans, Goodfellow, Zaremba, Cheung, Radford, and Chen}}}
\bibcite{simonyan2015vgg}{{27}{2015}{{Simonyan and Zisserman}}{{}}}
\bibcite{tantithamthavorn2020rebalancing}{{28}{2020}{{Tantithamthavorn et~al.}}{{Tantithamthavorn, Hassan, and Matsumoto}}}
\bibcite{welsh2002colorization}{{29}{2002}{{Welsh et~al.}}{{Welsh, Ashikhmin, and Mueller}}}
\bibcite{yu2016atrous}{{30}{2016}{{Yu and Koltun}}{{}}}
\bibcite{zhang2016colorful}{{31}{2016}{{Zhang et~al.}}{{Zhang, Isola, and Efros}}}
\bibcite{zhang2017ideep}{{32}{2017}{{Zhang et~al.}}{{Zhang, Zhu, Isola, Geng, Lin, Yu, and Efros}}}
\bibcite{zhou2017places}{{33}{2017}{{Zhou et~al.}}{{Zhou, Lapedriza, Khosla, Oliva, and Torralba}}}
\bibstyle{plainnat}
